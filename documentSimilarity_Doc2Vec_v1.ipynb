{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "random.seed(0)\n",
    "from pprint import pprint  # pretty-printer\n",
    "from collections import defaultdict # word counter\n",
    "#print(os.listdir(\".\"))\n",
    "\n",
    "# sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "np.random.seed(0)\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>User_Id</th>\n",
       "      <th>User_Name</th>\n",
       "      <th>User_Screen_Name</th>\n",
       "      <th>Theme</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1253375593271394312</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Registering for online access and activating y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1253322747851296768</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Protect your CARES Act payments: Validate comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1253066045700681731</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>It’s simple to set up a payment account in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1252960358454767616</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>We’re committed to helping provide the support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1252673469852143618</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Protect Yourself from COVID-19 Scams: Don’t re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tweet_Id   User_Id User_Name User_Screen_Name           Theme  \\\n",
       "0  1253375593271394312  80374332  Citibank         Citibank  Bank/Financial   \n",
       "1  1253322747851296768  80374332  Citibank         Citibank  Bank/Financial   \n",
       "2  1253066045700681731  80374332  Citibank         Citibank  Bank/Financial   \n",
       "3  1252960358454767616  80374332  Citibank         Citibank  Bank/Financial   \n",
       "4  1252673469852143618  80374332  Citibank         Citibank  Bank/Financial   \n",
       "\n",
       "                                                Text  \n",
       "0  Registering for online access and activating y...  \n",
       "1  Protect your CARES Act payments: Validate comm...  \n",
       "2  It’s simple to set up a payment account in the...  \n",
       "3  We’re committed to helping provide the support...  \n",
       "4  Protect Yourself from COVID-19 Scams: Don’t re...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = ['./tweets_04232020.csv', './tweets_04242020.csv', './tweets_04252020.csv', './tweets_04262020.csv', './tweets_04292020.csv', './tweets_04302020.csv']\n",
    "\n",
    "df_list = []\n",
    "for filename in all_files:\n",
    "    df_list.append(pd.read_csv(filename, sep=',',header=0, encoding='utf-8', index_col = 0))\n",
    "    \n",
    "df_tweets = pd.concat(df_list, ignore_index=True)\n",
    "df_tweets.head()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7700, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.drop_duplicates([\"Text\"], inplace=True) # remove duplicates in place and reset index\n",
    "df_tweets = df_tweets.reset_index(drop=True)\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['text'] = df['text'].str.replace('http\\S+', '') # removing URLs\n",
    "    df['text'] = df['text'].str.replace('[^A-Za-z]+', ' ') # retain only alphanumeric\n",
    "    df['text'] = df['text'].map(lambda x: x.lower()) # to lower case\n",
    "    df['text'] = df['text'].map(lambda x: WordNetLemmatizer().lemmatize(x)) # lemmatization   \n",
    "    df['text'] = df['text'].map(lambda x: word_tokenize(x)) # tokenize words\n",
    "    df['text'] = df['text'].map(lambda x: [word for word in x if word not in stop_words]) # remove stop words \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================= Raw text =============================================\n",
      "\n",
      " Registering for online access and activating your Citi® card is easy in the Citi Mobile® App. Learn more in the video below. https://t.co/Ec1rIUVX0l\n",
      "\n",
      "======================================== Preprocessed text =========================================\n",
      "\n",
      " ['registering', 'online', 'access', 'activating', 'citi', 'card', 'easy', 'citi', 'mobile', 'app', 'learn', 'video']\n"
     ]
    }
   ],
   "source": [
    "query_index = 0\n",
    "df = pd.DataFrame()\n",
    "df['text'] = df_tweets['Text']\n",
    "print('{:=^100}\\n\\n {}'.format(' Raw text ', df.text.loc[query_index])) \n",
    "\n",
    "df = preprocess(df)\n",
    "print('\\n{:=^100}\\n\\n {}'.format(' Preprocessed text ', df.text.loc[query_index])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 7700 \n",
      "Train records: 7546 \n",
      " Test records: 154\n"
     ]
    }
   ],
   "source": [
    "total_indices_size = df.shape[0]\n",
    "train_size = 0.98\n",
    "train_indices_size = int(train_size * total_indices_size)\n",
    "test_indices_size = total_indices_size - train_indices_size\n",
    "print('Total records: {} \\nTrain records: {} \\n Test records: {}'.format(total_indices_size, train_indices_size, test_indices_size))\n",
    "\n",
    "total_indices_array = np.array([x for x in range(total_indices_size)])\n",
    "np.random.shuffle(total_indices_array)\n",
    "train_indices, test_indices = total_indices_array[:train_indices_size], total_indices_array[train_indices_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7546, 1) (154, 1)\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = df.loc[train_indices], df.loc[test_indices]\n",
    "print(train_set.shape, test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['registering',\n",
       " 'online',\n",
       " 'access',\n",
       " 'activating',\n",
       " 'citi',\n",
       " 'card',\n",
       " 'easy',\n",
       " 'citi',\n",
       " 'mobile',\n",
       " 'app',\n",
       " 'learn',\n",
       " 'video']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.loc[0, \"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag Documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_documents(df):   \n",
    "    df_indices = df.index.to_list()      \n",
    "    for i in range(df.shape[0]):\n",
    "        yield TaggedDocument(df.loc[df_indices[i], \"text\"], [df_indices[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['penelopeonthego', 'lindseyboylan', 'heerjeet', 'perfect', 'guy', 'go', 'lying', 'dog', 'faced', 'pony', 'soldier', 'guy'], tags=[2241])]\n"
     ]
    }
   ],
   "source": [
    "train_corpus = list(tag_documents(train_set))\n",
    "pprint(train_corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['rt', 'blackpinkfml', 'jennie', 'apart', 'protect', 'family', 'abandoned', 'dog', 'adoption', 'campaign', 'since', 'donated', 'alot', 'supplie'], tags=[6242])]\n"
     ]
    }
   ],
   "source": [
    "test_corpus = list(tag_documents(test_set))\n",
    "pprint(test_corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=100, min_count=2, epochs=40)\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17554526 -0.06015937 -0.08083551 -0.03107999 -0.16151412 -0.21280283\n",
      "  0.02225097 -0.00215548 -0.19113825  0.06573348  0.02756668  0.10615117\n",
      "  0.09008887  0.09060719  0.0042705   0.0968703  -0.10958012  0.10060077\n",
      "  0.1920451  -0.00855945 -0.01824979  0.10325608 -0.32036293 -0.01775225\n",
      "  0.06614857 -0.12968439  0.0542037   0.03963283  0.0479433   0.07808828\n",
      "  0.03196692  0.03435186  0.06628725 -0.03315438  0.11596455  0.05443031\n",
      " -0.09314461  0.13202398  0.02717078 -0.08545028 -0.10960717 -0.03497319\n",
      " -0.07082701 -0.03742527  0.01837434 -0.03016695 -0.12433695 -0.10806288\n",
      " -0.16194136  0.12176057  0.07192167  0.02410932  0.04107241  0.0583943\n",
      "  0.01048861 -0.11878662 -0.0889902  -0.00433639 -0.04975854 -0.04022327\n",
      " -0.10307834 -0.06266695 -0.1612618   0.0697923  -0.17655678  0.05534594\n",
      "  0.04645586  0.08291026 -0.02102945 -0.08578876 -0.08884513 -0.02174282\n",
      "  0.0674123   0.12275301  0.06646194 -0.13852721 -0.0189216   0.1968533\n",
      " -0.19540337  0.03114284 -0.01444465 -0.03642225  0.0846766  -0.01414012\n",
      " -0.17311263 -0.04091065  0.00129409 -0.01786172  0.09388441 -0.00898713\n",
      " -0.04156386  0.02942213 -0.04474728 -0.0149532  -0.0067249  -0.02031491\n",
      " -0.05747001  0.08118825 -0.09901826 -0.00812796]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['i', 'love', 'espresso', 'coffee'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer Vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7546, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus_infer_list = [] \n",
    "for i in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[i].words)\n",
    "    train_corpus_infer_list.append(inferred_vector)\n",
    "    \n",
    "train_corpus_matrix = np.matrix(train_corpus_infer_list) \n",
    "train_corpus_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus_infer_list = [] \n",
    "for i in range(len(test_corpus)):\n",
    "    inferred_vector = model.infer_vector(test_corpus[i].words)\n",
    "    test_corpus_infer_list.append(inferred_vector)\n",
    "    \n",
    "test_corpus_matrix = np.matrix(test_corpus_infer_list) \n",
    "test_corpus_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Similar Comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similar_texts(query_text, query_text_vector,  train_matrix, train_indices, df_tweets, n = 5):\n",
    "    cosine_similarities_n = cosine_similarity(query_text_vector, train_matrix).flatten()\n",
    "    top_Indices = cosine_similarities_n.argsort()[::-1][:n]\n",
    "    top_tweet_Indices = train_indices[top_Indices]\n",
    "    \n",
    "    print('\\nInput Text:\\n {} \\n'.format(query_text))\n",
    "    for index, sim_text in enumerate(df_tweets.loc[top_tweet_Indices, \"Text\"]):\n",
    "        print('=' * 30, 'Similar Text: {} || Similar Score: {}'.format(index+1, np.round(cosine_similarities_n[top_Indices[index]], 3)), '=' * 30, '\\n')\n",
    "        print(sim_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Text:\n",
      " @81diosab I am sorry for the delayed response. Due to COVID-19 causing higher volume, clients are experiencing longer wait times worldwide. Were you able to reach Customer Service? If not, DM the account/card type only. No PIN or account numbers. ^Deanna https://t.co/gvC4jB6b0K \n",
      "\n",
      "============================== Similar Text: 1 || Similar Score: 0.6880000233650208 ============================== \n",
      "\n",
      "@On2Whls Let us review this further with you. We're sorry for the delayed response. Due to higher volumes during COVID-19, clients are experiencing longer wait times worldwide. DM the full name and type of account/card only. No PIN or account numbers. ^Deanna https://t.co/gvC4jAOA9c \n",
      "\n",
      "============================== Similar Text: 2 || Similar Score: 0.5730000138282776 ============================== \n",
      "\n",
      "@coolmaneesh Thanks for your patience, and I am sorry for the delay. Please DM your account type. No account numbers in the reply please. ^Brian https://t.co/gvC4jAOA9c \n",
      "\n",
      "============================== Similar Text: 3 || Similar Score: 0.5019999742507935 ============================== \n",
      "\n",
      "@JinWang34339222 Hi. Please check your DM. ^Ana \n",
      "\n",
      "============================== Similar Text: 4 || Similar Score: 0.492000013589859 ============================== \n",
      "\n",
      "@FlyinGaurav Due to COVID-19 impact causing higher volumes worldwide, please DM the concerns, account/card type only, and the country where it was open. Do not reply with any PIN or account numbers. ^Deanna https://t.co/gvC4jAOA9c \n",
      "\n",
      "============================== Similar Text: 5 || Similar Score: 0.4869999885559082 ============================== \n",
      "\n",
      "@macycwilliams Hi Macy. While we took precautionary measures, after consulting with the San Mateo Department of Health, it has been determined that two employees didn’t have direct contact with anyone of concern and normal operations have resumed. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_index = 33\n",
    "query_text = df_tweets.loc[test_indices[query_index], \"Text\"]\n",
    "query_text_vector = test_corpus_matrix[query_index]\n",
    "print_similar_texts(query_text, query_text_vector, train_corpus_matrix, train_indices, df_tweets, n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify the goodnes of similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_n(train_matrix, train_indices, test_matrix, test_indices, test_set_df, df_tweets, n=5):\n",
    "    similar_texts_list = []\n",
    "    similar_scores_list = []\n",
    "    similar_top_score_list = []\n",
    "    similar_themes_list = []\n",
    "    similar_top_themes_list = []\n",
    "    for vector in test_matrix:\n",
    "        cosine_similarities_n = cosine_similarity(vector, train_matrix).flatten()\n",
    "        similar_doc_indices =  cosine_similarities_n.argsort()[::-1][:n]\n",
    "        top_tweet_Indices = train_indices[similar_doc_indices]\n",
    "        \n",
    "        similar_texts = [text for text in df_tweets.loc[top_tweet_Indices, \"Text\"]]\n",
    "        similar_texts_list.append(similar_texts)\n",
    "        \n",
    "        similar_scores = [np.round(score, 3) for score in cosine_similarities_n[similar_doc_indices]]\n",
    "        similar_scores_list.append(similar_scores)\n",
    "        similar_top_score_list.append(similar_scores[0])\n",
    "                                                                   \n",
    "        similar_themes = [theme for theme in df_tweets.loc[top_tweet_Indices, \"Theme\"]]\n",
    "        similar_themes_list.append(similar_themes)\n",
    "        \n",
    "        top_similar_theme = max(set(similar_themes), key=lambda x: similar_themes.count(x))\n",
    "        similar_top_themes_list.append(top_similar_theme)\n",
    "    \n",
    "    df = test_set_df.copy() \n",
    "    df[\"original_theme\"] = df_tweets.loc[test_indices, \"Theme\"]\n",
    "    df[\"similar_texts\"] = similar_texts_list\n",
    "    df[\"similar_scores\"] = similar_scores_list  \n",
    "    df[\"top_similar_score\"] = similar_top_score_list\n",
    "    df[\"similar_themes\"] = similar_themes_list\n",
    "    df[\"top_similar_themes\"] = similar_top_themes_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>original_theme</th>\n",
       "      <th>similar_texts</th>\n",
       "      <th>similar_scores</th>\n",
       "      <th>top_similar_score</th>\n",
       "      <th>similar_themes</th>\n",
       "      <th>top_similar_themes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6242</th>\n",
       "      <td>[rt, blackpinkfml, jennie, apart, protect, fam...</td>\n",
       "      <td>dog</td>\n",
       "      <td>[@pulte In need of help with Credit Card debt ...</td>\n",
       "      <td>[0.6, 0.552, 0.54, 0.539, 0.531, 0.529, 0.524,...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>[dog, dog, dog, dog, dog, dog, cake, dog, dog,...</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6521</th>\n",
       "      <td>[ericawerner, feliciasonmez, long, term, care,...</td>\n",
       "      <td>airlines</td>\n",
       "      <td>[@MichaelRapaport @NYCMayor Don’t forget about...</td>\n",
       "      <td>[0.677, 0.667, 0.664, 0.658, 0.636, 0.632, 0.6...</td>\n",
       "      <td>0.677</td>\n",
       "      <td>[airlines, airlines, airlines, airlines, airli...</td>\n",
       "      <td>airlines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4226</th>\n",
       "      <td>[wsnt, car, back, damnit, wan, na, listen, new...</td>\n",
       "      <td>car</td>\n",
       "      <td>[RT @mitchellvii: I'm willing to take my chanc...</td>\n",
       "      <td>[0.509, 0.487, 0.484, 0.478, 0.455, 0.453, 0.4...</td>\n",
       "      <td>0.509</td>\n",
       "      <td>[covid-19, dog, car, coffee, car, Bank/Financi...</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5819</th>\n",
       "      <td>[hijonramirez, hello, thank, reaching, better,...</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>[@JoseJrq777 Hi Jose, thank you for reaching o...</td>\n",
       "      <td>[0.945, 0.942, 0.936, 0.927, 0.924, 0.922, 0.9...</td>\n",
       "      <td>0.945</td>\n",
       "      <td>[Bank/Financial, Bank/Financial, Bank/Financia...</td>\n",
       "      <td>Bank/Financial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6167</th>\n",
       "      <td>[rt, houseofbakes, th, raspberry, chocolate, m...</td>\n",
       "      <td>cake</td>\n",
       "      <td>[RT @LexiTriplet: For my 11th birthday, those ...</td>\n",
       "      <td>[0.82, 0.815, 0.815, 0.809, 0.798, 0.786, 0.78...</td>\n",
       "      <td>0.820</td>\n",
       "      <td>[cake, cake, cake, ramadan, Bank/Financial, ca...</td>\n",
       "      <td>cake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  original_theme  \\\n",
       "6242  [rt, blackpinkfml, jennie, apart, protect, fam...             dog   \n",
       "6521  [ericawerner, feliciasonmez, long, term, care,...        airlines   \n",
       "4226  [wsnt, car, back, damnit, wan, na, listen, new...             car   \n",
       "5819  [hijonramirez, hello, thank, reaching, better,...  Bank/Financial   \n",
       "6167  [rt, houseofbakes, th, raspberry, chocolate, m...            cake   \n",
       "\n",
       "                                          similar_texts  \\\n",
       "6242  [@pulte In need of help with Credit Card debt ...   \n",
       "6521  [@MichaelRapaport @NYCMayor Don’t forget about...   \n",
       "4226  [RT @mitchellvii: I'm willing to take my chanc...   \n",
       "5819  [@JoseJrq777 Hi Jose, thank you for reaching o...   \n",
       "6167  [RT @LexiTriplet: For my 11th birthday, those ...   \n",
       "\n",
       "                                         similar_scores  top_similar_score  \\\n",
       "6242  [0.6, 0.552, 0.54, 0.539, 0.531, 0.529, 0.524,...              0.600   \n",
       "6521  [0.677, 0.667, 0.664, 0.658, 0.636, 0.632, 0.6...              0.677   \n",
       "4226  [0.509, 0.487, 0.484, 0.478, 0.455, 0.453, 0.4...              0.509   \n",
       "5819  [0.945, 0.942, 0.936, 0.927, 0.924, 0.922, 0.9...              0.945   \n",
       "6167  [0.82, 0.815, 0.815, 0.809, 0.798, 0.786, 0.78...              0.820   \n",
       "\n",
       "                                         similar_themes top_similar_themes  \n",
       "6242  [dog, dog, dog, dog, dog, dog, cake, dog, dog,...                dog  \n",
       "6521  [airlines, airlines, airlines, airlines, airli...           airlines  \n",
       "4226  [covid-19, dog, car, coffee, car, Bank/Financi...                car  \n",
       "5819  [Bank/Financial, Bank/Financial, Bank/Financia...     Bank/Financial  \n",
       "6167  [cake, cake, cake, ramadan, Bank/Financial, ca...               cake  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_test_df = find_similar_n(train_corpus_matrix, train_indices, test_corpus_matrix, test_indices, test_set, df_tweets, n=20)\n",
    "similar_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data in predicting theme: 77.27%\n"
     ]
    }
   ],
   "source": [
    "accuracy = 100 * np.sum(similar_test_df[\"original_theme\"] == similar_test_df[\"top_similar_themes\"])/similar_test_df.shape[0]\n",
    "print(\"Accuracy on test data in predicting theme: {}%\".format(np.round(accuracy,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_test_df.to_csv(\"Doc2Vec_v1_similar_test_tweets.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
