{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'BankTweets.csv', 'BankTweets_04242020.csv', 'BankTweets_04252020.csv', 'BankTweets_04262020.csv', 'BankTweets_04292020.csv', 'BankTweets_04302020.csv', 'cbikkanur_twitter_credentials.ipynb', 'cbikkanur_twitter_credentials.json', 'CosineSimilarity.ipynb', 'documentSimilarity.ipynb', 'documentSimilarity_TfIdf.ipynb', 'documentSimilarity_TfIdf.ipynb.txt', 'documentSimilarity_TfIdf_v2.ipynb', 'getTwitterData.ipynb', 'getTwitterData_TweePy.ipynb', 'model_save', 'SearchTweets.csv', 'SearchTweets_04242020.csv', 'SearchTweets_04252020.csv', 'SearchTweets_04262020.csv', 'SearchTweets_04292020.csv', 'SearchTweets_04302020.csv', 'test_vectors.npy', 'TfIdf_similar_test_tweets.csv', 'TfIdf_v2_similar_test_tweets.csv', 'train_vectors.npy', 'tweets_04232020.csv', 'tweets_04242020.csv', 'tweets_04252020.csv', 'tweets_04262020.csv', 'tweets_04292020.csv', 'tweets_04302020.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vencxbikkanur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "print(os.listdir(\".\"))\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "np.random.seed(0)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>User_Id</th>\n",
       "      <th>User_Name</th>\n",
       "      <th>User_Screen_Name</th>\n",
       "      <th>Theme</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1253375593271394312</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Registering for online access and activating y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1253322747851296768</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Protect your CARES Act payments: Validate comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1253066045700681731</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Itâ€™s simple to set up a payment account in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1252960358454767616</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Weâ€™re committed to helping provide the support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1252673469852143618</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Protect Yourself from COVID-19 Scams: Donâ€™t re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tweet_Id   User_Id User_Name User_Screen_Name           Theme  \\\n",
       "0  1253375593271394312  80374332  Citibank         Citibank  Bank/Financial   \n",
       "1  1253322747851296768  80374332  Citibank         Citibank  Bank/Financial   \n",
       "2  1253066045700681731  80374332  Citibank         Citibank  Bank/Financial   \n",
       "3  1252960358454767616  80374332  Citibank         Citibank  Bank/Financial   \n",
       "4  1252673469852143618  80374332  Citibank         Citibank  Bank/Financial   \n",
       "\n",
       "                                                Text  \n",
       "0  Registering for online access and activating y...  \n",
       "1  Protect your CARES Act payments: Validate comm...  \n",
       "2  Itâ€™s simple to set up a payment account in the...  \n",
       "3  Weâ€™re committed to helping provide the support...  \n",
       "4  Protect Yourself from COVID-19 Scams: Donâ€™t re...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = ['./tweets_04232020.csv', './tweets_04242020.csv', './tweets_04252020.csv', './tweets_04262020.csv', './tweets_04292020.csv', './tweets_04302020.csv']\n",
    "\n",
    "df_list = []\n",
    "for filename in all_files:\n",
    "    df_list.append(pd.read_csv(filename, sep=',',header=0, encoding='utf-8', index_col = 0))\n",
    "    \n",
    "df_tweets = pd.concat(df_list, ignore_index=True)\n",
    "df_tweets.head()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7700, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.drop_duplicates([\"Text\"], inplace=True) # remove duplicates in place and reset index\n",
    "df_tweets = df_tweets.reset_index(drop=True)\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================= Raw text =============================================\n",
      "\n",
      " To our heroes going out into the world each day â€“ thank you. Citi is supporting COVID-19 relief efforts around the world to help those bravely showing up for the rest of us. https://t.co/oaknGpLLlX https://t.co/wRZ7o6lupM\n",
      "\n",
      "======================================== Preprocessed text =========================================\n",
      "\n",
      " to our heroes going out into the world each day thank you citi is supporting covid 19 relief efforts around the world to help those bravely showing up for the rest of us \n"
     ]
    }
   ],
   "source": [
    "query_index = 10\n",
    "df = pd.DataFrame()\n",
    "df['text'] = df_tweets['Text']\n",
    "print('{:=^100}\\n\\n {}'.format(' Raw text ', df.text.loc[query_index])) \n",
    "\n",
    "df['text'] = df['text'].str.replace('http\\S+', '') # removing URLs\n",
    "df['text'] = df['text'].str.replace('[^A-Za-z0-9]+', ' ') # retain only alphanumeric\n",
    "df['text'] = df['text'].map(lambda x: WordNetLemmatizer().lemmatize(x)) # lemmatization\n",
    "df['text'] = df['text'].map(lambda x: x.lower()) # to lower case\n",
    "\n",
    "print('\\n{:=^100}\\n\\n {}'.format(' Preprocessed text ', df.text.loc[query_index])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 7700 \n",
      "Train records: 6160 \n",
      " Test records: 1540\n"
     ]
    }
   ],
   "source": [
    "total_indices_size = df.shape[0]\n",
    "train_size = 0.8\n",
    "train_indices_size = int(train_size * total_indices_size)\n",
    "test_indices_size = total_indices_size - train_indices_size\n",
    "print('Total records: {} \\nTrain records: {} \\n Test records: {}'.format(total_indices_size, train_indices_size, test_indices_size))\n",
    "\n",
    "total_indices_array = np.array([x for x in range(total_indices_size)])\n",
    "np.random.shuffle(total_indices_array)\n",
    "train_indices, test_indices = total_indices_array[:train_indices_size], total_indices_array[train_indices_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6160, 1) (1540, 1)\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = df.loc[train_indices], df.loc[test_indices]\n",
    "print(train_set.shape, test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer & Tf-Idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words = 'english', ngram_range=(1,3), \\\n",
    "                                   strip_accents='unicode', \\\n",
    "                                   analyzer = 'word', \\\n",
    "                                   lowercase = True)\n",
    "tfidf = TfidfTransformer()\n",
    "pipeline = Pipeline(steps=[('count_vectorizer', count_vectorizer), ('tfidf', tfidf)])\n",
    "\n",
    "tf_idf_train_matrix = pipeline.fit_transform(train_set[\"text\"])\n",
    "tf_idf_test_matrix = pipeline.transform(test_set[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Matrix Shape: (6160, 126789)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Matrix Shape:\", tf_idf_train_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similar_texts(query_text, query_text_vector,  train_matrix, train_indices, df_tweets, n = 5):\n",
    "    cosine_similarities_n = cosine_similarity(query_text_vector, train_matrix).flatten()\n",
    "    top_Indices = cosine_similarities_n.argsort()[::-1][:n]\n",
    "    top_tweet_Indices = train_indices[top_Indices]\n",
    "    \n",
    "    print('\\nInput Text:\\n {} \\n'.format(query_text))\n",
    "    for index, sim_text in enumerate(df_tweets.loc[top_tweet_Indices, \"Text\"]):\n",
    "        print('=' * 30, 'Similar Text: {} || Similar Score: {}'.format(index+1, np.round(cosine_similarities_n[top_Indices[index]], 3)), '=' * 30, '\\n')\n",
    "        print(sim_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Text:\n",
      " NASA Presents Trump with NEW Space Grade Ventilator and Fog Disinfectant... https://t.co/ZKQN8FtLXQ via @YouTube \n",
      "\n",
      "============================== Similar Text: 1 || Similar Score: 1.0 ============================== \n",
      "\n",
      "NASA Presents Trump with NEW Space Grade Ventilator and Fog Disinfectant... https://t.co/cDKqTVyD0d via @YouTube \n",
      "\n",
      "============================== Similar Text: 2 || Similar Score: 0.586 ============================== \n",
      "\n",
      "RT @IrmaBel53130008: NASA Presents Trump with NEW Space Grade Ventilator &amp; Fog Disinfectant to Combat the Coronavirus!  How great will thisâ€¦ \n",
      "\n",
      "============================== Similar Text: 3 || Similar Score: 0.56 ============================== \n",
      "\n",
      "RT @Wyn1745: NASA Presents Trump with NEW Space Grade Ventilator &amp; Fog Disinfectant to Combat #Coronavirus- #ChinaVirus #COVID19\n",
      "\n",
      "This willâ€¦ \n",
      "\n",
      "============================== Similar Text: 4 || Similar Score: 0.385 ============================== \n",
      "\n",
      "ðŸ˜Ž NASA presents @realDonaldTrump\n",
      "with Space grade Ventilator!\n",
      "\n",
      "https://t.co/M0VweCsPxQ \n",
      "\n",
      "============================== Similar Text: 5 || Similar Score: 0.069 ============================== \n",
      "\n",
      "@ClaudiaMyTime @ABC Trump Steaks, Trump University, Trump Ice, Trump Airlines, Trump Magazine...should I keep going? ðŸ¤¦ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_index = 33\n",
    "query_text = df_tweets.loc[test_indices[query_index], \"Text\"]\n",
    "query_text_vector = tf_idf_test_matrix[query_index]\n",
    "print_similar_texts(query_text, query_text_vector, tf_idf_train_matrix, train_indices, df_tweets, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_n(tf_idf_train_matrix, train_indices, tf_idf_test_matrix, test_indices, test_set_df, df_tweets, n=5):\n",
    "    similar_texts_list = []\n",
    "    similar_scores_list = []\n",
    "    similar_top_score_list = []\n",
    "    similar_themes_list = []\n",
    "    similar_top_themes_list = []\n",
    "    for vector in tf_idf_test_matrix:\n",
    "        cosine_similarities_n = cosine_similarity(vector, tf_idf_train_matrix).flatten()\n",
    "        similar_doc_indices =  cosine_similarities_n.argsort()[::-1][:n]\n",
    "        top_tweet_Indices = train_indices[similar_doc_indices]\n",
    "        \n",
    "        similar_texts = [text for text in df_tweets.loc[top_tweet_Indices, \"Text\"]]\n",
    "        similar_texts_list.append(similar_texts)\n",
    "        \n",
    "        similar_scores = [np.round(score, 3) for score in cosine_similarities_n[similar_doc_indices]]\n",
    "        similar_scores_list.append(similar_scores)\n",
    "        similar_top_score_list.append(similar_scores[0])\n",
    "                                                                   \n",
    "        similar_themes = [theme for theme in df_tweets.loc[top_tweet_Indices, \"Theme\"]]\n",
    "        similar_themes_list.append(similar_themes)\n",
    "        \n",
    "        top_similar_theme = max(set(similar_themes), key=lambda x: similar_themes.count(x))\n",
    "        similar_top_themes_list.append(top_similar_theme)\n",
    "    \n",
    "    df = test_set_df \n",
    "    df[\"original_theme\"] = df_tweets.loc[test_indices, \"Theme\"]\n",
    "    df[\"similar_texts\"] = similar_texts_list\n",
    "    df[\"similar_scores\"] = similar_scores_list  \n",
    "    df[\"top_similar_score\"] = similar_top_score_list\n",
    "    df[\"similar_themes\"] = similar_themes_list\n",
    "    df[\"top_similar_themes\"] = similar_top_themes_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_test_df = find_similar_n(tf_idf_train_matrix, train_indices, tf_idf_test_matrix, test_indices, test_set, df_tweets, n=20)\n",
    "similar_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 100 * np.sum(similar_test_df[\"original_theme\"] == similar_test_df[\"top_similar_themes\"])/similar_test_df.shape[0]\n",
    "print(\"Accuracy on test data in predicting theme: {}%\".format(np.round(accuracy,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_test_df.to_csv(\"TfIdf_v2_similar_test_tweets.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
