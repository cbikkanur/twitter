{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from pprint import pprint  # pretty-printer\n",
    "from collections import defaultdict # word counter\n",
    "#print(os.listdir(\".\"))\n",
    "\n",
    "# sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "np.random.seed(0)\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>User_Id</th>\n",
       "      <th>User_Name</th>\n",
       "      <th>User_Screen_Name</th>\n",
       "      <th>Theme</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1253375593271394312</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Registering for online access and activating y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1253322747851296768</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Protect your CARES Act payments: Validate comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1253066045700681731</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>It’s simple to set up a payment account in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1252960358454767616</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>We’re committed to helping provide the support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1252673469852143618</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Protect Yourself from COVID-19 Scams: Don’t re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tweet_Id   User_Id User_Name User_Screen_Name           Theme  \\\n",
       "0  1253375593271394312  80374332  Citibank         Citibank  Bank/Financial   \n",
       "1  1253322747851296768  80374332  Citibank         Citibank  Bank/Financial   \n",
       "2  1253066045700681731  80374332  Citibank         Citibank  Bank/Financial   \n",
       "3  1252960358454767616  80374332  Citibank         Citibank  Bank/Financial   \n",
       "4  1252673469852143618  80374332  Citibank         Citibank  Bank/Financial   \n",
       "\n",
       "                                                Text  \n",
       "0  Registering for online access and activating y...  \n",
       "1  Protect your CARES Act payments: Validate comm...  \n",
       "2  It’s simple to set up a payment account in the...  \n",
       "3  We’re committed to helping provide the support...  \n",
       "4  Protect Yourself from COVID-19 Scams: Don’t re...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = ['./tweets_04232020.csv', './tweets_04242020.csv', './tweets_04252020.csv', './tweets_04262020.csv', './tweets_04292020.csv', './tweets_04302020.csv']\n",
    "\n",
    "df_list = []\n",
    "for filename in all_files:\n",
    "    df_list.append(pd.read_csv(filename, sep=',',header=0, encoding='utf-8', index_col = 0))\n",
    "    \n",
    "df_tweets = pd.concat(df_list, ignore_index=True)\n",
    "df_tweets.head()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7700, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.drop_duplicates([\"Text\"], inplace=True) # remove duplicates in place and reset index\n",
    "df_tweets = df_tweets.reset_index(drop=True)\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['text'] = df['text'].str.replace('http\\S+', '') # removing URLs\n",
    "    df['text'] = df['text'].str.replace('[^A-Za-z]+', ' ') # retain only alphanumeric\n",
    "    df['text'] = df['text'].map(lambda x: x.lower()) # to lower case\n",
    "    df['text'] = df['text'].map(lambda x: WordNetLemmatizer().lemmatize(x)) # lemmatization   \n",
    "    df['text'] = df['text'].map(lambda x: word_tokenize(x)) # tokenize words\n",
    "    df['text'] = df['text'].map(lambda x: [word for word in x if word not in stop_words]) # remove stop words \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================= Raw text =============================================\n",
      "\n",
      " Registering for online access and activating your Citi® card is easy in the Citi Mobile® App. Learn more in the video below. https://t.co/Ec1rIUVX0l\n",
      "\n",
      "======================================== Preprocessed text =========================================\n",
      "\n",
      " ['registering', 'online', 'access', 'activating', 'citi', 'card', 'easy', 'citi', 'mobile', 'app', 'learn', 'video']\n"
     ]
    }
   ],
   "source": [
    "query_index = 0\n",
    "df = pd.DataFrame()\n",
    "df['text'] = df_tweets['Text']\n",
    "print('{:=^100}\\n\\n {}'.format(' Raw text ', df.text.loc[query_index])) \n",
    "\n",
    "df = preprocess(df)\n",
    "print('\\n{:=^100}\\n\\n {}'.format(' Preprocessed text ', df.text.loc[query_index])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 7700 \n",
      "Train records: 7546 \n",
      " Test records: 154\n"
     ]
    }
   ],
   "source": [
    "total_indices_size = df.shape[0]\n",
    "train_size = 0.98\n",
    "train_indices_size = int(train_size * total_indices_size)\n",
    "test_indices_size = total_indices_size - train_indices_size\n",
    "print('Total records: {} \\nTrain records: {} \\n Test records: {}'.format(total_indices_size, train_indices_size, test_indices_size))\n",
    "\n",
    "total_indices_array = np.array([x for x in range(total_indices_size)])\n",
    "np.random.shuffle(total_indices_array)\n",
    "train_indices, test_indices = total_indices_array[:train_indices_size], total_indices_array[train_indices_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7546, 1) (154, 1)\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = df.loc[train_indices], df.loc[test_indices]\n",
    "print(train_set.shape, test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary & Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(19457 unique tokens: ['dog', 'faced', 'go', 'guy', 'heerjeet']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(train_set['text'])\n",
    "dictionary.save('./tweets.dict')  \n",
    "print(dictionary)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in train_set['text']]\n",
    "corpora.MmCorpus.serialize('./tweets_corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.save('./tweets.index')\n",
    "index = similarities.MatrixSimilarity.load('./tweets.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Text:\n",
      " covid-19 pandemic \n",
      "\n",
      "============================== Similar Text: 1 || Similar Score: 0.8320000171661377 ============================== \n",
      "\n",
      "This #EarthDay feels different than others. We examine the other ways the pandemic has taken a toll on nature despite lower carbon emissions.  https://t.co/KkzyjIakoI \n",
      "\n",
      "============================== Similar Text: 2 || Similar Score: 0.6420000195503235 ============================== \n",
      "\n",
      "@hopd87 @Citibank The pandemic has challenged us all. We're trying our best to look out for clients and our employees. I appreciate your patience w/ our delayed response. Send a DM us with your account type/brand and the country it was opened. Never send acct numbers or PINs. ^Mike \n",
      "\n",
      "============================== Similar Text: 3 || Similar Score: 0.640999972820282 ============================== \n",
      "\n",
      "@FK1tten The pandemic has challenged us all. We're trying our best to look out for clients and our employees. I appreciate your patience w/ our delayed response. Send a DM us with your account type/brand and the country it was opened. Never send acct numbers or PINs. ^Mike \n",
      "\n",
      "============================== Similar Text: 4 || Similar Score: 0.6399999856948853 ============================== \n",
      "\n",
      "@Steve_Font The pandemic has challenged us all. We're trying our best to look out for clients and our employees. I appreciate your patience w/ our delayed response. Send a DM us with your account type/brand and the country it was opened. Never send acct numbers or PINs. ^Mike \n",
      "\n",
      "============================== Similar Text: 5 || Similar Score: 0.6399999856948853 ============================== \n",
      "\n",
      "@PRAVINKHANVILK8 The pandemic has challenged us all. We're trying our best to look out for clients and our employees. I appreciate your patience w/ our delayed response. Send a DM us with your account type/brand and the country it was opened. Never send acct numbers or PINs. ^Mike \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "n = 5\n",
    "doc = \"covid-19 pandemic\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
    "sims = index[vec_lsi]  # perform a similarity query against the corpus\n",
    "top_Indices = sims.argsort()[::-1][:n]\n",
    "top_tweet_Indices = train_indices[top_Indices]\n",
    "\n",
    "print('\\nInput Text:\\n {} \\n'.format(doc))\n",
    "for i, sim_text in enumerate(df_tweets.loc[top_tweet_Indices, \"Text\"]):\n",
    "        print('=' * 30, 'Similar Text: {} || Similar Score: {}'.format(i+1, np.round(sims[top_Indices[i]], 3)), '=' * 30, '\\n')\n",
    "        print(sim_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similar_texts(query_text, query_text_vector,  index, train_indices, df_tweets, n = 5):\n",
    "    sims = index[query_text_vector]  # perform a similarity query against the corpus\n",
    "    top_Indices = sims.argsort()[::-1][:n]\n",
    "    top_tweet_Indices = train_indices[top_Indices]\n",
    "\n",
    "    print('\\nInput Text:\\n {} \\n'.format(query_text))\n",
    "    for index, sim_text in enumerate(df_tweets.loc[top_tweet_Indices, \"Text\"]):\n",
    "        print('=' * 30, 'Similar Text: {} || Similar Score: {}'.format(index+1, np.round(sims[top_Indices[index]], 3)), '=' * 30, '\\n')\n",
    "        print(sim_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Text:\n",
      " Listened to the news in my car and sports news lasted 10 seconds. 🤣 \n",
      "\n",
      "============================== Similar Text: 1 || Similar Score: 0.7910000085830688 ============================== \n",
      "\n",
      "Jennifer Aniston shared the quarantine parody of the cute Friends theme song | Instant News https://t.co/geFZUQ9y0W https://t.co/05MlyEoI6h \n",
      "\n",
      "============================== Similar Text: 2 || Similar Score: 0.7369999885559082 ============================== \n",
      "\n",
      "@helvo6 The big car behind you🛣 \n",
      "\n",
      "============================== Similar Text: 3 || Similar Score: 0.718999981880188 ============================== \n",
      "\n",
      "Update he just put a car in neutral and drove it down the garage \n",
      "\n",
      "============================== Similar Text: 4 || Similar Score: 0.6489999890327454 ============================== \n",
      "\n",
      "I always wonder why my old car gave up on my ass, but than makayla reminds me how I use to dick it on a daily 😂😂😂 https://t.co/uHl6vYLmW7 \n",
      "\n",
      "============================== Similar Text: 5 || Similar Score: 0.6349999904632568 ============================== \n",
      "\n",
      "NO MORE bad news .. not on here or in DM’s or texts emails or phone. Not by FedX car bus train horse &amp; buggy or a sign in my yard. Not by TV newspaper Instagram FB Facetime radio or drones. In short don’t relay ANY bad news to ME.  BILLIONS of good things out there. FIND ONE \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_index = 27\n",
    "query_text = df_tweets.loc[test_indices[query_index], \"Text\"]\n",
    "query_text_list = df.loc[test_indices[query_index], \"text\"]\n",
    "query_vec_bow = dictionary.doc2bow(query_text_list)\n",
    "query_text_vector = lsi[query_vec_bow] \n",
    "print_similar_texts(query_text, query_text_vector,  index, train_indices, df_tweets, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_n(train_index_lsi, train_indices, test_set, test_indices, test_set_df, df_tweets, n=5):\n",
    "    similar_texts_list = []\n",
    "    similar_scores_list = []\n",
    "    similar_top_score_list = []\n",
    "    similar_themes_list = []\n",
    "    similar_top_themes_list = []\n",
    "    for vector in test_set.text:\n",
    "        #print(vector)\n",
    "        query_vec_bow = dictionary.doc2bow(vector)\n",
    "        sims = train_index_lsi[lsi[query_vec_bow]]  # perform a similarity query against the corpus\n",
    "        top_Indices = sims.argsort()[::-1][:n]\n",
    "        top_tweet_Indices = train_indices[top_Indices]\n",
    "                \n",
    "        similar_texts = [text for text in df_tweets.loc[top_tweet_Indices, \"Text\"]]\n",
    "        similar_texts_list.append(similar_texts)\n",
    "        \n",
    "        similar_scores = [np.round(score, 3) for score in sims[top_Indices]]\n",
    "        similar_scores_list.append(similar_scores)\n",
    "        similar_top_score_list.append(similar_scores[0])\n",
    "                                                                   \n",
    "        similar_themes = [theme for theme in df_tweets.loc[top_tweet_Indices, \"Theme\"]]\n",
    "        similar_themes_list.append(similar_themes)\n",
    "        \n",
    "        top_similar_theme = max(set(similar_themes), key=lambda x: similar_themes.count(x))\n",
    "        similar_top_themes_list.append(top_similar_theme)\n",
    "    \n",
    "    df = test_set_df.copy() \n",
    "    df[\"original_theme\"] = df_tweets.loc[test_indices, \"Theme\"]\n",
    "    df[\"similar_texts\"] = similar_texts_list\n",
    "    df[\"similar_scores\"] = similar_scores_list  \n",
    "    df[\"top_similar_score\"] = similar_top_score_list\n",
    "    df[\"similar_themes\"] = similar_themes_list\n",
    "    df[\"top_similar_themes\"] = similar_top_themes_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>original_theme</th>\n",
       "      <th>similar_texts</th>\n",
       "      <th>similar_scores</th>\n",
       "      <th>top_similar_score</th>\n",
       "      <th>similar_themes</th>\n",
       "      <th>top_similar_themes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6242</th>\n",
       "      <td>[rt, blackpinkfml, jennie, apart, protect, fam...</td>\n",
       "      <td>dog</td>\n",
       "      <td>[RT @jory_c505: Wienerschnitzel makes a fine c...</td>\n",
       "      <td>[0.986, 0.986, 0.985, 0.985, 0.985, 0.985, 0.9...</td>\n",
       "      <td>0.986</td>\n",
       "      <td>[dog, dog, dog, dog, dog, dog, dog, dog, dog, ...</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6521</th>\n",
       "      <td>[ericawerner, feliciasonmez, long, term, care,...</td>\n",
       "      <td>airlines</td>\n",
       "      <td>[Now playing Low Cost Airlines  by ! https://t...</td>\n",
       "      <td>[0.84, 0.838, 0.837, 0.837, 0.836, 0.836, 0.83...</td>\n",
       "      <td>0.840</td>\n",
       "      <td>[airlines, airlines, airlines, airlines, airli...</td>\n",
       "      <td>airlines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4226</th>\n",
       "      <td>[wsnt, car, back, damnit, wan, na, listen, new...</td>\n",
       "      <td>car</td>\n",
       "      <td>[new car seat headrest is so gooooood, *TOYOTA...</td>\n",
       "      <td>[0.789, 0.775, 0.743, 0.734, 0.726, 0.721, 0.7...</td>\n",
       "      <td>0.789</td>\n",
       "      <td>[car, car, car, car, car, car, car, car, car, ...</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5819</th>\n",
       "      <td>[hijonramirez, hello, thank, reaching, better,...</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>[@JoseJrq777 Hi Jose, thank you for reaching o...</td>\n",
       "      <td>[0.961, 0.956, 0.952, 0.951, 0.94, 0.93, 0.925...</td>\n",
       "      <td>0.961</td>\n",
       "      <td>[Bank/Financial, Bank/Financial, Bank/Financia...</td>\n",
       "      <td>Bank/Financial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6167</th>\n",
       "      <td>[rt, houseofbakes, th, raspberry, chocolate, m...</td>\n",
       "      <td>cake</td>\n",
       "      <td>[RT @LexiTriplet: For my 11th birthday, those ...</td>\n",
       "      <td>[0.987, 0.98, 0.977, 0.977, 0.975, 0.974, 0.97...</td>\n",
       "      <td>0.987</td>\n",
       "      <td>[cake, cake, cake, cake, cake, cake, cake, cak...</td>\n",
       "      <td>cake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  original_theme  \\\n",
       "6242  [rt, blackpinkfml, jennie, apart, protect, fam...             dog   \n",
       "6521  [ericawerner, feliciasonmez, long, term, care,...        airlines   \n",
       "4226  [wsnt, car, back, damnit, wan, na, listen, new...             car   \n",
       "5819  [hijonramirez, hello, thank, reaching, better,...  Bank/Financial   \n",
       "6167  [rt, houseofbakes, th, raspberry, chocolate, m...            cake   \n",
       "\n",
       "                                          similar_texts  \\\n",
       "6242  [RT @jory_c505: Wienerschnitzel makes a fine c...   \n",
       "6521  [Now playing Low Cost Airlines  by ! https://t...   \n",
       "4226  [new car seat headrest is so gooooood, *TOYOTA...   \n",
       "5819  [@JoseJrq777 Hi Jose, thank you for reaching o...   \n",
       "6167  [RT @LexiTriplet: For my 11th birthday, those ...   \n",
       "\n",
       "                                         similar_scores  top_similar_score  \\\n",
       "6242  [0.986, 0.986, 0.985, 0.985, 0.985, 0.985, 0.9...              0.986   \n",
       "6521  [0.84, 0.838, 0.837, 0.837, 0.836, 0.836, 0.83...              0.840   \n",
       "4226  [0.789, 0.775, 0.743, 0.734, 0.726, 0.721, 0.7...              0.789   \n",
       "5819  [0.961, 0.956, 0.952, 0.951, 0.94, 0.93, 0.925...              0.961   \n",
       "6167  [0.987, 0.98, 0.977, 0.977, 0.975, 0.974, 0.97...              0.987   \n",
       "\n",
       "                                         similar_themes top_similar_themes  \n",
       "6242  [dog, dog, dog, dog, dog, dog, dog, dog, dog, ...                dog  \n",
       "6521  [airlines, airlines, airlines, airlines, airli...           airlines  \n",
       "4226  [car, car, car, car, car, car, car, car, car, ...                car  \n",
       "5819  [Bank/Financial, Bank/Financial, Bank/Financia...     Bank/Financial  \n",
       "6167  [cake, cake, cake, cake, cake, cake, cake, cak...               cake  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_test_df = find_similar_n(index, train_indices, test_set, test_indices, test_set, df_tweets, n=20)\n",
    "similar_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data in predicting theme: 87.01%\n"
     ]
    }
   ],
   "source": [
    "accuracy = 100 * np.sum(similar_test_df[\"original_theme\"] == similar_test_df[\"top_similar_themes\"])/similar_test_df.shape[0]\n",
    "print(\"Accuracy on test data in predicting theme: {}%\".format(np.round(accuracy,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_test_df.to_csv(\"LSI_v1_similar_test_tweets.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
