{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from pprint import pprint  # pretty-printer\n",
    "from collections import defaultdict # word counter\n",
    "#print(os.listdir(\".\"))\n",
    "\n",
    "# sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "np.random.seed(0)\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>User_Id</th>\n",
       "      <th>User_Name</th>\n",
       "      <th>User_Screen_Name</th>\n",
       "      <th>Theme</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1253375593271394312</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Registering for online access and activating y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1253322747851296768</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Protect your CARES Act payments: Validate comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1253066045700681731</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Itâ€™s simple to set up a payment account in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1252960358454767616</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Weâ€™re committed to helping provide the support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1252673469852143618</td>\n",
       "      <td>80374332</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>Protect Yourself from COVID-19 Scams: Donâ€™t re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tweet_Id   User_Id User_Name User_Screen_Name           Theme  \\\n",
       "0  1253375593271394312  80374332  Citibank         Citibank  Bank/Financial   \n",
       "1  1253322747851296768  80374332  Citibank         Citibank  Bank/Financial   \n",
       "2  1253066045700681731  80374332  Citibank         Citibank  Bank/Financial   \n",
       "3  1252960358454767616  80374332  Citibank         Citibank  Bank/Financial   \n",
       "4  1252673469852143618  80374332  Citibank         Citibank  Bank/Financial   \n",
       "\n",
       "                                                Text  \n",
       "0  Registering for online access and activating y...  \n",
       "1  Protect your CARES Act payments: Validate comm...  \n",
       "2  Itâ€™s simple to set up a payment account in the...  \n",
       "3  Weâ€™re committed to helping provide the support...  \n",
       "4  Protect Yourself from COVID-19 Scams: Donâ€™t re...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = ['./tweets_04232020.csv', './tweets_04242020.csv', './tweets_04252020.csv', './tweets_04262020.csv', './tweets_04292020.csv', './tweets_04302020.csv']\n",
    "\n",
    "df_list = []\n",
    "for filename in all_files:\n",
    "    df_list.append(pd.read_csv(filename, sep=',',header=0, encoding='utf-8', index_col = 0))\n",
    "    \n",
    "df_tweets = pd.concat(df_list, ignore_index=True)\n",
    "df_tweets.head()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7700, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.drop_duplicates([\"Text\"], inplace=True) # remove duplicates in place and reset index\n",
    "df_tweets = df_tweets.reset_index(drop=True)\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['text'] = df['text'].str.replace('http\\S+', '') # removing URLs\n",
    "    df['text'] = df['text'].str.replace('[^A-Za-z]+', ' ') # retain only alphanumeric\n",
    "    df['text'] = df['text'].map(lambda x: x.lower()) # to lower case\n",
    "    df['text'] = df['text'].map(lambda x: WordNetLemmatizer().lemmatize(x)) # lemmatization   \n",
    "    df['text'] = df['text'].map(lambda x: word_tokenize(x)) # tokenize words\n",
    "    df['text'] = df['text'].map(lambda x: [word for word in x if word not in stop_words]) # remove stop words \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================= Raw text =============================================\n",
      "\n",
      " Registering for online access and activating your CitiÂ® card is easy in the Citi MobileÂ® App. Learn more in the video below. https://t.co/Ec1rIUVX0l\n",
      "\n",
      "======================================== Preprocessed text =========================================\n",
      "\n",
      " ['registering', 'online', 'access', 'activating', 'citi', 'card', 'easy', 'citi', 'mobile', 'app', 'learn', 'video']\n"
     ]
    }
   ],
   "source": [
    "query_index = 0\n",
    "df = pd.DataFrame()\n",
    "df['text'] = df_tweets['Text']\n",
    "print('{:=^100}\\n\\n {}'.format(' Raw text ', df.text.loc[query_index])) \n",
    "\n",
    "df = preprocess(df)\n",
    "print('\\n{:=^100}\\n\\n {}'.format(' Preprocessed text ', df.text.loc[query_index])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 7700 \n",
      "Train records: 7546 \n",
      " Test records: 154\n"
     ]
    }
   ],
   "source": [
    "total_indices_size = df.shape[0]\n",
    "train_size = 0.98\n",
    "train_indices_size = int(train_size * total_indices_size)\n",
    "test_indices_size = total_indices_size - train_indices_size\n",
    "print('Total records: {} \\nTrain records: {} \\n Test records: {}'.format(total_indices_size, train_indices_size, test_indices_size))\n",
    "\n",
    "total_indices_array = np.array([x for x in range(total_indices_size)])\n",
    "np.random.shuffle(total_indices_array)\n",
    "train_indices, test_indices = total_indices_array[:train_indices_size], total_indices_array[train_indices_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7546, 1) (154, 1)\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = df.loc[train_indices], df.loc[test_indices]\n",
    "print(train_set.shape, test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary & Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(19457 unique tokens: ['dog', 'faced', 'go', 'guy', 'heerjeet']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(train_set['text'])\n",
    "dictionary.save('./tweets.dict')  \n",
    "print(dictionary)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in train_set['text']]\n",
    "corpora.MmCorpus.serialize('./tweets_corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.save('./tweets.index')\n",
    "index = similarities.MatrixSimilarity.load('./tweets.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Text:\n",
      " covid-19 pandemic \n",
      "\n",
      "============================== Similar Text: 1 || Similar Score: 0.8320000171661377 ============================== \n",
      "\n",
      "This #EarthDay feels different than others. We examine the other ways the pandemic has taken a toll on nature despite lower carbon emissions.  https://t.co/KkzyjIakoI \n",
      "\n",
      "============================== Similar Text: 2 || Similar Score: 0.6420000195503235 ============================== \n",
      "\n",
      "@hopd87 @Citibank The pandemic has challenged us all. We're trying our best to look out for clients and our employees. I appreciate your patience w/ our delayed response. Send a DM us with your account type/brand and the country it was opened. Never send acct numbers or PINs. ^Mike \n",
      "\n",
      "============================== Similar Text: 3 || Similar Score: 0.640999972820282 ============================== \n",
      "\n",
      "@FK1tten The pandemic has challenged us all. We're trying our best to look out for clients and our employees. I appreciate your patience w/ our delayed response. Send a DM us with your account type/brand and the country it was opened. Never send acct numbers or PINs. ^Mike \n",
      "\n",
      "============================== Similar Text: 4 || Similar Score: 0.6399999856948853 ============================== \n",
      "\n",
      "@Steve_Font The pandemic has challenged us all. We're trying our best to look out for clients and our employees. I appreciate your patience w/ our delayed response. Send a DM us with your account type/brand and the country it was opened. Never send acct numbers or PINs. ^Mike \n",
      "\n",
      "============================== Similar Text: 5 || Similar Score: 0.6399999856948853 ============================== \n",
      "\n",
      "@PRAVINKHANVILK8 The pandemic has challenged us all. We're trying our best to look out for clients and our employees. I appreciate your patience w/ our delayed response. Send a DM us with your account type/brand and the country it was opened. Never send acct numbers or PINs. ^Mike \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "n = 5\n",
    "doc = \"covid-19 pandemic\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
    "sims = index[vec_lsi]  # perform a similarity query against the corpus\n",
    "top_Indices = sims.argsort()[::-1][:n]\n",
    "top_tweet_Indices = train_indices[top_Indices]\n",
    "\n",
    "print('\\nInput Text:\\n {} \\n'.format(doc))\n",
    "for i, sim_text in enumerate(df_tweets.loc[top_tweet_Indices, \"Text\"]):\n",
    "        print('=' * 30, 'Similar Text: {} || Similar Score: {}'.format(i+1, np.round(sims[top_Indices[i]], 3)), '=' * 30, '\\n')\n",
    "        print(sim_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similar_texts(query_text, query_text_vector,  index, train_indices, df_tweets, n = 5):\n",
    "    sims = index[query_text_vector]  # perform a similarity query against the corpus\n",
    "    top_Indices = sims.argsort()[::-1][:n]\n",
    "    top_tweet_Indices = train_indices[top_Indices]\n",
    "\n",
    "    print('\\nInput Text:\\n {} \\n'.format(query_text))\n",
    "    for index, sim_text in enumerate(df_tweets.loc[top_tweet_Indices, \"Text\"]):\n",
    "        print('=' * 30, 'Similar Text: {} || Similar Score: {}'.format(index+1, np.round(sims[top_Indices[index]], 3)), '=' * 30, '\\n')\n",
    "        print(sim_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Text:\n",
      " Listened to the news in my car and sports news lasted 10 seconds. ðŸ¤£ \n",
      "\n",
      "============================== Similar Text: 1 || Similar Score: 0.7910000085830688 ============================== \n",
      "\n",
      "Jennifer Aniston shared the quarantine parody of the cute Friends theme song | InstantÂ News https://t.co/geFZUQ9y0W https://t.co/05MlyEoI6h \n",
      "\n",
      "============================== Similar Text: 2 || Similar Score: 0.7369999885559082 ============================== \n",
      "\n",
      "@helvo6 The big car behind youðŸ›£ \n",
      "\n",
      "============================== Similar Text: 3 || Similar Score: 0.718999981880188 ============================== \n",
      "\n",
      "Update he just put a car in neutral and drove it down the garage \n",
      "\n",
      "============================== Similar Text: 4 || Similar Score: 0.6489999890327454 ============================== \n",
      "\n",
      "I always wonder why my old car gave up on my ass, but than makayla reminds me how I use to dick it on a daily ðŸ˜‚ðŸ˜‚ðŸ˜‚ https://t.co/uHl6vYLmW7 \n",
      "\n",
      "============================== Similar Text: 5 || Similar Score: 0.6349999904632568 ============================== \n",
      "\n",
      "NO MORE bad news .. not on here or in DMâ€™s or texts emails or phone. Not by FedX car bus train horse &amp; buggy or a sign in my yard. Not by TV newspaper Instagram FB Facetime radio or drones. In short donâ€™t relay ANY bad news to ME.  BILLIONS of good things out there. FIND ONE \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_index = 27\n",
    "query_text = df_tweets.loc[test_indices[query_index], \"Text\"]\n",
    "query_text_list = df.loc[test_indices[query_index], \"text\"]\n",
    "query_vec_bow = dictionary.doc2bow(query_text_list)\n",
    "query_text_vector = lsi[query_vec_bow] \n",
    "print_similar_texts(query_text, query_text_vector,  index, train_indices, df_tweets, n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_n(train_index_lsi, train_indices, test_set, test_indices, test_set_df, df_tweets, n=5):\n",
    "    similar_texts_list = []\n",
    "    similar_scores_list = []\n",
    "    similar_top_score_list = []\n",
    "    similar_themes_list = []\n",
    "    similar_top_themes_list = []\n",
    "    for vector in test_set.text:\n",
    "        #print(vector)\n",
    "        query_vec_bow = dictionary.doc2bow(vector)\n",
    "        sims = train_index_lsi[lsi[query_vec_bow]]  # perform a similarity query against the corpus\n",
    "        top_Indices = sims.argsort()[::-1][:n]\n",
    "        top_tweet_Indices = train_indices[top_Indices]\n",
    "                \n",
    "        similar_texts = [text for text in df_tweets.loc[top_tweet_Indices, \"Text\"]]\n",
    "        similar_texts_list.append(similar_texts)\n",
    "        \n",
    "        similar_scores = [np.round(score, 3) for score in sims[top_Indices]]\n",
    "        similar_scores_list.append(similar_scores)\n",
    "        similar_top_score_list.append(similar_scores[0])\n",
    "                                                                   \n",
    "        similar_themes = [theme for theme in df_tweets.loc[top_tweet_Indices, \"Theme\"]]\n",
    "        similar_themes_list.append(similar_themes)\n",
    "        \n",
    "        top_similar_theme = max(set(similar_themes), key=lambda x: similar_themes.count(x))\n",
    "        similar_top_themes_list.append(top_similar_theme)\n",
    "    \n",
    "    df = test_set_df.copy() \n",
    "    df[\"original_theme\"] = df_tweets.loc[test_indices, \"Theme\"]\n",
    "    df[\"similar_texts\"] = similar_texts_list\n",
    "    df[\"similar_scores\"] = similar_scores_list  \n",
    "    df[\"top_similar_score\"] = similar_top_score_list\n",
    "    df[\"similar_themes\"] = similar_themes_list\n",
    "    df[\"top_similar_themes\"] = similar_top_themes_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>original_theme</th>\n",
       "      <th>similar_texts</th>\n",
       "      <th>similar_scores</th>\n",
       "      <th>top_similar_score</th>\n",
       "      <th>similar_themes</th>\n",
       "      <th>top_similar_themes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6242</th>\n",
       "      <td>[rt, blackpinkfml, jennie, apart, protect, fam...</td>\n",
       "      <td>dog</td>\n",
       "      <td>[RT @jory_c505: Wienerschnitzel makes a fine c...</td>\n",
       "      <td>[0.986, 0.986, 0.985, 0.985, 0.985, 0.985, 0.9...</td>\n",
       "      <td>0.986</td>\n",
       "      <td>[dog, dog, dog, dog, dog, dog, dog, dog, dog, ...</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6521</th>\n",
       "      <td>[ericawerner, feliciasonmez, long, term, care,...</td>\n",
       "      <td>airlines</td>\n",
       "      <td>[Now playing Low Cost Airlines  by ! https://t...</td>\n",
       "      <td>[0.84, 0.838, 0.837, 0.837, 0.836, 0.836, 0.83...</td>\n",
       "      <td>0.840</td>\n",
       "      <td>[airlines, airlines, airlines, airlines, airli...</td>\n",
       "      <td>airlines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4226</th>\n",
       "      <td>[wsnt, car, back, damnit, wan, na, listen, new...</td>\n",
       "      <td>car</td>\n",
       "      <td>[new car seat headrest is so gooooood, *TOYOTA...</td>\n",
       "      <td>[0.789, 0.775, 0.743, 0.734, 0.726, 0.721, 0.7...</td>\n",
       "      <td>0.789</td>\n",
       "      <td>[car, car, car, car, car, car, car, car, car, ...</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5819</th>\n",
       "      <td>[hijonramirez, hello, thank, reaching, better,...</td>\n",
       "      <td>Bank/Financial</td>\n",
       "      <td>[@JoseJrq777 Hi Jose, thank you for reaching o...</td>\n",
       "      <td>[0.961, 0.956, 0.952, 0.951, 0.94, 0.93, 0.925...</td>\n",
       "      <td>0.961</td>\n",
       "      <td>[Bank/Financial, Bank/Financial, Bank/Financia...</td>\n",
       "      <td>Bank/Financial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6167</th>\n",
       "      <td>[rt, houseofbakes, th, raspberry, chocolate, m...</td>\n",
       "      <td>cake</td>\n",
       "      <td>[RT @LexiTriplet: For my 11th birthday, those ...</td>\n",
       "      <td>[0.987, 0.98, 0.977, 0.977, 0.975, 0.974, 0.97...</td>\n",
       "      <td>0.987</td>\n",
       "      <td>[cake, cake, cake, cake, cake, cake, cake, cak...</td>\n",
       "      <td>cake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  original_theme  \\\n",
       "6242  [rt, blackpinkfml, jennie, apart, protect, fam...             dog   \n",
       "6521  [ericawerner, feliciasonmez, long, term, care,...        airlines   \n",
       "4226  [wsnt, car, back, damnit, wan, na, listen, new...             car   \n",
       "5819  [hijonramirez, hello, thank, reaching, better,...  Bank/Financial   \n",
       "6167  [rt, houseofbakes, th, raspberry, chocolate, m...            cake   \n",
       "\n",
       "                                          similar_texts  \\\n",
       "6242  [RT @jory_c505: Wienerschnitzel makes a fine c...   \n",
       "6521  [Now playing Low Cost Airlines  by ! https://t...   \n",
       "4226  [new car seat headrest is so gooooood, *TOYOTA...   \n",
       "5819  [@JoseJrq777 Hi Jose, thank you for reaching o...   \n",
       "6167  [RT @LexiTriplet: For my 11th birthday, those ...   \n",
       "\n",
       "                                         similar_scores  top_similar_score  \\\n",
       "6242  [0.986, 0.986, 0.985, 0.985, 0.985, 0.985, 0.9...              0.986   \n",
       "6521  [0.84, 0.838, 0.837, 0.837, 0.836, 0.836, 0.83...              0.840   \n",
       "4226  [0.789, 0.775, 0.743, 0.734, 0.726, 0.721, 0.7...              0.789   \n",
       "5819  [0.961, 0.956, 0.952, 0.951, 0.94, 0.93, 0.925...              0.961   \n",
       "6167  [0.987, 0.98, 0.977, 0.977, 0.975, 0.974, 0.97...              0.987   \n",
       "\n",
       "                                         similar_themes top_similar_themes  \n",
       "6242  [dog, dog, dog, dog, dog, dog, dog, dog, dog, ...                dog  \n",
       "6521  [airlines, airlines, airlines, airlines, airli...           airlines  \n",
       "4226  [car, car, car, car, car, car, car, car, car, ...                car  \n",
       "5819  [Bank/Financial, Bank/Financial, Bank/Financia...     Bank/Financial  \n",
       "6167  [cake, cake, cake, cake, cake, cake, cake, cak...               cake  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_test_df = find_similar_n(index, train_indices, test_set, test_indices, test_set, df_tweets, n=20)\n",
    "similar_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data in predicting theme: 87.01%\n"
     ]
    }
   ],
   "source": [
    "accuracy = 100 * np.sum(similar_test_df[\"original_theme\"] == similar_test_df[\"top_similar_themes\"])/similar_test_df.shape[0]\n",
    "print(\"Accuracy on test data in predicting theme: {}%\".format(np.round(accuracy,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_test_df.to_csv(\"LSI_v1_similar_test_tweets.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
